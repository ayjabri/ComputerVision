{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP with pytorch book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation and target encoding\n",
    "### One hot encoding representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#https://towardsdatascience.com/building-a-sentiment-classifier-using-scikit-learn-54c8e7c5d2f0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONE HOT ENCODING\n",
    "# Consider these two sentences \n",
    "sentence_1 = 'Time flies like and arrow'\n",
    "sentence_2 = 'Fruit flies like a banana'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi my name is Ayman Al jabri.',\n",
       " 'I work for Citigroup as a businesss analyst.',\n",
       " 'I work New York City.',\n",
       " 'I Live in Fort Lee NJ']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenizing a paragraph with multiple sentences\n",
    "parag = \"\"\"Hi my name is Ayman Al jabri.\n",
    "I work for Citigroup as a businesss analyst. I work New York City. I Live in Fort Lee NJ\"\"\"\n",
    "\n",
    "tokenized = nltk.sent_tokenize(parag)\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_sentence1 = [1,1,0,1,1,0,1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 0, 1, 1]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# import seaborn as sns\n",
    "\n",
    "\n",
    "corpus = ['Fruit flies like time flies a fruit','time flies like an arrow']\n",
    "one_hot_vector = CountVectorizer(binary=True)\n",
    "one_hot = one_hot_vector.fit_transform(corpus).toarray()\n",
    "one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fecf1c08890>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD4CAYAAADM6gxlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAKmUlEQVR4nO3ba6hl9XnH8d9zvFCxXkgscW5lFKWVVqJl4huJJFLvt4GA2qKBIkiIgYSCJoVAEHLxjUP1VZCkudBWI6QlpYW2IU1RaVJHo7He2sYocS5RQitWEDpN/n0xR2vqZMYxe581z5rPBw5z9l6cWc/DmfNlnbX31BgjAPSxMvUAABwc4QZoRrgBmhFugGaEG6CZI5d+gqM3eNtKU6/uun/qEeCwddRJp9YvOuaKG6AZ4QZoRrgBmhFugGaEG6AZ4QZoRrgBmhFugGaEG6AZ4QZoRrgBmhFugGaEG6AZ4QZoRrgBmhFugGaEG6AZ4QZoRrgBmhFugGaEG6AZ4QZoRrgBmhFugGaEG6AZ4QZoRrgBmhFugGaEG6AZ4QZoRrgBmhFugGaEG6AZ4QZoRrgBmhFugGaEG6AZ4QZoRrgBmhFugGaEG6AZ4QZoRrgBmhFugGaEG6AZ4QZoRrgBmhFugGaEG6AZ4QZoZlbhvujC9+WJx+/L008+kFtuvmnqcRZqzrslySc/uy3nXXZttl73oalHWbg575bYbwqzCffKykruvOMzufyK63Lmu9+fa67ZmjPOOH3qsRZizru9ZuulF+Tz2z499RhLMefdEvtN4YDhrqrfrKqPV9WdVXXH6udnrMVwB+Oc95ydZ555Ls8++6Ps2bMn9977jVx5xUVTj7UQc97tNVvOOjMnHH/c1GMsxZx3S+w3hf2Gu6o+nuSeJJXkwSTbVz+/u6o+sfzx3rr1G07O8zt2vf54x87dWb/+5AknWpw57wYcvCMPcPyGJL81xtjzxieraluSJ5Lctq8vqqobk9yYJHXECVlZOXYBo+5fVb3puTHG0s+7Fua8G3DwDnSr5GdJ1u/j+XWrx/ZpjHHXGGPLGGPLWkQ7SXbu2J1NG/9v1I0b1mX37hfW5NzLNufdgIN3oCvujyX5VlX9e5LnV5/79SSnJfnIMgc7WNsfejSnnXZKNm/elJ07f5yrr74q139wHu++mPNuwMGrA/3KXVUrSc5JsiF772/vSLJ9jPHTt3KCI4/esGa/019y8fm5/fZbc8TKSr78la/lc7fduVanXropdnt11/1LP8drbv7Ubdn+yGN56aWX8853nJgP33B9PjCTF2DnvFtiv2U56qRT33yPdNUBw/3LWstws1hrGW7g5+0v3LN5HzfA4UK4AZoRboBmhBugGeEGaEa4AZoRboBmhBugGeEGaEa4AZoRboBmhBugGeEGaEa4AZoRboBmhBugGeEGaEa4AZoRboBmhBugGeEGaEa4AZoRboBmhBugGeEGaEa4AZoRboBmhBugGeEGaEa4AZoRboBmhBugGeEGaEa4AZoRboBmhBugGeEGaEa4AZoRboBmhBugGeEGaEa4AZoRboBmhBugGeEGaEa4AZoRboBmhBugGeEGaObIZZ/g1V33L/sU8LYcs/69U4+wVH725ssVN0Azwg3QjHADNCPcAM0IN0Azwg3QjHADNCPcAM0IN0Azwg3QjHADNCPcAM0IN0Azwg3QjHADNCPcAM0IN0Azwg3QjHADNCPcAM0IN0Azwg3QjHADNCPcAM0IN0Azwg3QjHADNCPcAM0IN0Azwg3QjHADNCPcAM0IN0Azwg3QjHADNCPcAM0IN0Azwg3QjHADNCPcAM0IN0Azwg3QjHADNCPcAM0IN0Azwg3QjHADNCPcAM0IN0Azswn3Jz+7Ledddm22XvehqUdZCvv1dtGF78sTj9+Xp598ILfcfNPU4yzU3L93h+J+swn31ksvyOe3fXrqMZbGfn2trKzkzjs+k8uvuC5nvvv9ueaarTnjjNOnHmth5vy9Sw7N/WYT7i1nnZkTjj9u6jGWxn59nfOes/PMM8/l2Wd/lD179uTee7+RK6+4aOqxFmbO37vk0NxvNuGGQ9X6DSfn+R27Xn+8Y+furF9/8oQT0d3bDndV/cF+jt1YVQ9V1UNf+Ordb/cUMAtV9abnxhgTTMJcHPlLfO2tSb60rwNjjLuS3JUke37yQ/9COazt3LE7mzauf/3xxg3rsnv3CxNORHf7DXdVPfaLDiV51+LHgfnZ/tCjOe20U7J586bs3PnjXH31Vbn+g/N6Zwlrq/b3K1tVvZDkoiT/+f8PJfmnMcb6N3/Vz1urK+6bP3Vbtj/yWF566eW88x0n5sM3XJ8PzOgFIPst3jHr37vUv/+NLrn4/Nx++605YmUlX/7K1/K52+5c+jlf3XX/0s+R+Le5LEeddOqb77GtOlC4v5jkS2OMB/Zx7M/HGL9/oJO7VcKhai3DPYW1CjfLsb9w7/dWyRjjhv0cO2C0AVg8bwcEaEa4AZoRboBmhBugGeEGaEa4AZoRboBmhBugGeEGaEa4AZoRboBmhBugGeEGaEa4AZoRboBmhBugGeEGaEa4AZoRboBmhBugGeEGaEa4AZoRboBmhBugGeEGaEa4AZoRboBmhBugGeEGaEa4AZoRboBmhBugGeEGaEa4AZoRboBmhBugGeEGaEa4AZoRboBmhBugGeEGaEa4AZoRboBmhBugGeEGaEa4AZoRboBmhBugGeEGaKbGGFPPsFBVdeMY466p51gW+/U25/3mvFtyaO03xyvuG6ceYMns19uc95vzbskhtN8cww0wa8IN0Mwcw31I3INaIvv1Nuf95rxbcgjtN7sXJwHmbo5X3ACzJtwAzcwq3FV1cVX9a1X9oKo+MfU8i1RVf1JVL1bV41PPsmhVtamqvl1VT1XVE1X10alnWqSq+pWqerCqvr+6361Tz7QMVXVEVT1SVX899SyLVlXPVdW/VNWjVfXQ5PPM5R53VR2R5N+SXJBkR5LtSX5vjPHkpIMtSFWdl+SVJF8dY/z21PMsUlWtS7JujPG9qjouycNJts7oe1dJjh1jvFJVRyV5IMlHxxjfnXi0haqqP0yyJcnxY4zLp55nkarquSRbxhg/mXqWZF5X3Ock+cEY44djjP9Ock+SqyaeaWHGGPcl+Y+p51iGMcbuMcb3Vj//ryRPJdkw7VSLM/Z6ZfXhUasf87hiWlVVG5NcluQLU89yOJhTuDckef4Nj3dkRj/8h4uq2pzk7CT/PO0ki7V6G+HRJC8m+eYYY1b7JfnjJLck+dnUgyzJSPL3VfVwVU3+PyjnFO7ax3OzuqqZu6r61SRfT/KxMcbLU8+zSGOMn44xzkqyMck5VTWb211VdXmSF8cYD089yxKdO8b4nSSXJLlp9dblZOYU7h1JNr3h8cYkuyaahYO0eu/360n+bIzxF1PPsyxjjJeS/GOSiyceZZHOTXLl6n3ge5KcX1V/Ou1IizXG2LX654tJ/jJ7b81OZk7h3p7k9Ko6paqOTnJtkr+aeCbegtUX776Y5Kkxxrap51m0qvq1qjpx9fNjkvxukqennWpxxhh/NMbYOMbYnL0/d/8wxrhu4rEWpqqOXX3RPFV1bJILk0z67q7ZhHuM8T9JPpLk77L3xa17xxhPTDvV4lTV3Um+k+Q3qmpHVd0w9UwLdG6S67P3Su3R1Y9Lpx5qgdYl+XZVPZa9FxjfHGPM7i1zM/auJA9U1feTPJjkb8YYfzvlQLN5OyDA4WI2V9wAhwvhBmhGuAGaEW6AZoQboBnhBmhGuAGa+V+SYZxA6ha55QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(one_hot,annot=True,cbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF & TF-IDF Representation:\n",
    "TF: Term Frequency representation; is the sum of one hot that represents how many times the word is mentioned in the sentence (corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 2, 2, 1, 1],\n",
       "       [1, 1, 1, 0, 1, 1]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TF_hot_vector = CountVectorizer()\n",
    "TF_hot = TF_hot_vector.fit_transform(corpus).toarray()\n",
    "TF_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.53641614, 0.75391417, 0.26820807,\n",
       "        0.26820807],\n",
       "       [0.53309782, 0.53309782, 0.37930349, 0.        , 0.37930349,\n",
       "        0.37930349]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#IDF(w) = log N/nw\n",
    "#it penalizes common tokens and rewards rare ones in vecotor representation\n",
    "TFD_vectorizer = TfidfVectorizer()\n",
    "TFD_hot = TFD_vectorizer.fit_transform(corpus).toarray()\n",
    "TFD_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Mary', ',', 'do', \"n't\", 'slap', 'the', 'green', 'witch'], 'do')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Mary, don't slap the green witch\"\n",
    "textToken = word_tokenize(text)\n",
    "textToken,textToken[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mary', ',', \"don't\", 'slap', 'the', 'green', 'witch']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TweetTokenizer()\n",
    "tokenizer.tokenize(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc =\"\"\"The Eurovision Song Contest was cancelled earlier this week because of the coronavirus outbreak - but organisers say an alternative event could be held.\n",
    "\n",
    "The European Broadcasting Union (EBU) says it is “exploring alternative programming” to honour the songs and artists chosen to represent the 41 competing nations.\n",
    "\n",
    "While this event would not be classed as a competition, the EBU believes it would \"help unite and entertain audiences around Europe during these challenging times\".\n",
    "\n",
    "It means this year's entries will not be eligible for the 2021 competition.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"I think this shot deserves one #FreeCallofDuty + Slice of pizza+ #CODPromo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'think',\n",
       " 'this',\n",
       " 'shot',\n",
       " 'deserves',\n",
       " 'one',\n",
       " '#FreeCallofDuty',\n",
       " '+',\n",
       " 'Slice',\n",
       " 'of',\n",
       " 'pizza',\n",
       " '+',\n",
       " '#CODPromo']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmas and Stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/aymanjabri/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n",
      "better : good\n"
     ]
    }
   ],
   "source": [
    "# import these modules \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "  \n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\")) \n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\")) \n",
    "  \n",
    "# a denotes adjective in \"pos\" \n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'automobile'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('automobiles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram wth NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.youtube.com/watch?v=IqPWZL5f-7g&list=PLcTXcpndN-Sl9eYrKM6jtcOTgC52EJnqH\n",
    "\n",
    "from nltk.collocations import BigramAssocMeasures,BigramCollocationFinder #Find Bigram\n",
    "from nltk.corpus import webtext # Contains a library of text files\n",
    "\n",
    "textwords = [w.lower() for w in webtext.words('pirates.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['pirates', 'of', 'the', 'carribean', ':', 'dead', 'man', \"'\", 's', 'chest'],\n",
       " 22679)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textwords[:10],len(textwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder = BigramCollocationFinder.from_words(textwords,window_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'\", 's'),\n",
       " ('jack', 'sparrow'),\n",
       " (']', '['),\n",
       " ('will', 'turner'),\n",
       " ('sparrow', ':'),\n",
       " ('elizabeth', 'swann'),\n",
       " ('turner', ':'),\n",
       " ('davy', 'jones'),\n",
       " ('swann', ':'),\n",
       " (\"'\", 't')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder.nbest(BigramAssocMeasures.likelihood_ratio,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to apply a filter to get rid of useless words:\n",
    "\n",
    "Import a list of STOP words from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "ignore_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ignore_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a lambda filter to exclude words less than three letters or in stopwords\n",
    "IgnoreFilter = lambda w: len(w)<3 or w in ignore_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we apply the filter function to finder function\n",
    "finder.apply_word_filter(IgnoreFilter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('jack', 'sparrow'),\n",
       " ('elizabeth', 'swann'),\n",
       " ('davy', 'jones'),\n",
       " ('flying', 'dutchman'),\n",
       " ('lord', 'cutler'),\n",
       " ('cutler', 'beckett'),\n",
       " ('black', 'pearl'),\n",
       " ('tia', 'dalma'),\n",
       " ('cannibal', 'island'),\n",
       " ('port', 'royal')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re run finder for nbest using likelihood ration\n",
    "finder.nbest(BigramAssocMeasures.likelihood_ratio,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import TrigramAssocMeasures,TrigramCollocationFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scene', '1', ':', '[', 'wind', ']', '[', 'clop', 'clop', 'clop']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textWords = [w.lower() for w in webtext.words('grail.txt')]\n",
    "textWords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[', 'boom', ']'),\n",
       " ('[', 'singing', ']'),\n",
       " ('[', 'music', ']'),\n",
       " ('[', 'clang', ']'),\n",
       " ('.', 'arthur', ':'),\n",
       " ('[', 'chanting', ']'),\n",
       " ('[', 'pause', ']'),\n",
       " ('[', 'squeak', ']'),\n",
       " ('[', 'thud', ']'),\n",
       " ('[', 'bonk', ']')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First we create a finder function:\n",
    "t_finder = TrigramCollocationFinder.from_words(textWords)\n",
    "\n",
    "# Second we apply n-best words using the Measure of choice\n",
    "t_finder.nbest(TrigramAssocMeasures.likelihood_ratio,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('clop', 'clop', 'clop'),\n",
       " ('mumble', 'mumble', 'mumble'),\n",
       " ('squeak', 'squeak', 'squeak'),\n",
       " ('saw', 'saw', 'saw'),\n",
       " ('black', 'knight', 'kills'),\n",
       " ('black', 'knight', 'always'),\n",
       " ('pie', 'iesu', 'domine'),\n",
       " ('clap', 'clap', 'clap'),\n",
       " ('squeak', 'squeak', '...]'),\n",
       " ('...', 'head', 'knight')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We apply the same labmda filter above\n",
    "t_finder.apply_word_filter(IgnoreFilter)\n",
    "\n",
    "t_finder.nbest(TrigramAssocMeasures.likelihood_ratio,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('[', 'boom', ']'), 2594.3499225967994),\n",
       " (('[', 'singing', ']'), 2537.8050570270143),\n",
       " (('[', 'music', ']'), 2468.738779238396),\n",
       " (('[', 'clang', ']'), 2415.0101832477076),\n",
       " (('.', 'arthur', ':'), 2343.8790644572255),\n",
       " (('[', 'chanting', ']'), 2268.2365317327767),\n",
       " (('[', 'pause', ']'), 2268.2365317327767),\n",
       " (('[', 'squeak', ']'), 2265.5967687533835),\n",
       " (('[', 'thud', ']'), 2259.8028693252004),\n",
       " (('[', 'bonk', ']'), 2254.982128624187)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_finder.score_ngrams(TrigramAssocMeasures.likelihood_ratio)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
