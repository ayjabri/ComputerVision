{\rtf1\ansi\ansicpg1252\cocoartf2512
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fnil\fcharset0 HelveticaNeue-Bold;\f1\fnil\fcharset0 HelveticaNeue;\f2\fnil\fcharset0 Verdana;
\f3\froman\fcharset0 Palatino-Italic;\f4\froman\fcharset0 Times-Italic;\f5\fnil\fcharset0 Verdana-Bold;
\f6\fswiss\fcharset0 Helvetica;\f7\fswiss\fcharset0 Helvetica-Bold;\f8\fnil\fcharset0 Verdana-BoldItalic;
\f9\froman\fcharset0 Palatino-Roman;\f10\froman\fcharset0 Palatino-Bold;\f11\fnil\fcharset0 LucidaGrande;
\f12\froman\fcharset0 Palatino-BoldItalic;\f13\fnil\fcharset0 AppleSymbols;\f14\fmodern\fcharset0 CourierNewPSMT;
\f15\fnil\fcharset0 HelveticaNeue-Italic;}
{\colortbl;\red255\green255\blue255;\red55\green55\blue55;\red255\green255\blue255;\red12\green96\blue165;
\red26\green26\blue26;\red37\green37\blue37;\red0\green0\blue0;\red251\green0\blue7;\red0\green0\blue255;
}
{\*\expandedcolortbl;;\cssrgb\c27843\c27843\c27843;\cssrgb\c100000\c100000\c100000;\cssrgb\c0\c45882\c70588;
\cssrgb\c13333\c13333\c13333;\cssrgb\c19216\c19216\c19216;\cssrgb\c0\c0\c0;\cssrgb\c100000\c0\c0;\cssrgb\c0\c0\c100000;
}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid1}
{\list\listtemplateid2\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid101\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{circle\}}{\leveltext\leveltemplateid102\'01\uc0\u9702 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid2}
{\list\listtemplateid3\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid201\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid3}
{\list\listtemplateid4\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid301\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid4}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}{\listoverride\listid2\listoverridecount0\ls2}{\listoverride\listid3\listoverridecount0\ls3}{\listoverride\listid4\listoverridecount0\ls4}}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\sl504\partightenfactor0

\f0\b\fs36 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 NLP\
\pard\pardeftab720\sl414\partightenfactor0

\fs28 \cf4 \strokec4 \'a0Bookmark this page
\f1\b0\fs32 \cf5 \strokec5 \
\pard\pardeftab720\sl448\sa453\partightenfactor0

\f0\b \cf6 \strokec6 INSTRUCTIONS
\f1\b0 \cb1 \
\pard\pardeftab720\sl448\sa453\partightenfactor0

\f2 \cf6 \cb3 Congratulation on making it to the last programming project. By coming this far, we assume that you have accumulated formidable knowledge in both traditional Artificial Intelligence (AI) and modern Machine Learning (ML), and from now on we will treat you as such. This assignment intends\'a0to give you a flavor of a real world AI/ML application, which often require to gather the raw data, do preprocessing, design suitable ML algorithms and\'a0implement the solution. Today, we touch on an active research area in Natural Language Processing (NLP), sentiment analysis.
\f1 \cb1 \

\f2 \cb3 Given the exponentially growing of online review data (Amazon, IMDB and etc), sentiment analysis becomes increasingly important. We are going to build a sentiment\'a0classifier, i.e., evaluating a piece of text being either positive or negative.
\f1 \cb1 \

\f2 \cb3 The "Large Movie Review Dataset"(*) shall\'a0be used for this project. The dataset is compiled from a collection of 50,000 reviews from IMDB on the condition there are no more than 30 reviews each movie.\'a0
\f1 Number
\f2 \'a0of positive and negative reviews are equal. Negative reviews have scores\'a0lesser or equal 4 out of 10 while a positive review greater or equal 7 out of 10. Neutral reviews are not included on the other hand. Then, 50,000 reviews are divided evenly into the training and test set.
\f1 \cb1 \
\pard\pardeftab720\sl448\sa453\partightenfactor0

\f3\i \cf6 \cb3 *Dataset is credited to Prof. Andrew Mass in the paper,\'a0
\fs24 \cf7 \strokec7 Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011).\'a0{\field{\*\fldinst{HYPERLINK "http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf"}}{\fldrslt 
\f4 \cf4 \strokec4 Learning Word Vectors for Sentiment Analysis.}}\'a0
\fs32 \cf6 \strokec6 The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011).
\f1\i0 \cb1 \
\pard\pardeftab720\sl448\partightenfactor0

\f5\b \cf8 \cb3 \strokec7 Due date:
\f6\b0 \cf7 \cb1 \
\pard\pardeftab720\sl440\partightenfactor0
\cf7 \
\pard\pardeftab720\sl448\partightenfactor0

\f7\b \cf8 \cb3 The assignment's final due date is 5/17/2020, 23:30 UTC.\'a0
\f6\b0 \cf7 \cb1 \
\pard\pardeftab720\sl512\sa453\partightenfactor0

\f1 \cf6 \strokec6 \
\pard\pardeftab720\sl440\partightenfactor0

\f6 \cf7 \strokec7 \
\pard\pardeftab720\sl512\sa453\partightenfactor0

\f1 \cf6 \strokec6 \
\pard\pardeftab720\sl537\sa200\partightenfactor0

\f5\b\fs38\fsmilli19200 \cf6 \cb3 I.\'a0Instruction
\f0 \cb1 \
\pard\pardeftab720\sl512\sa453\partightenfactor0

\f1\b0\fs32 \cf6 \cb3 Up until now, most of the course projects have\'a0been requiring you to implement algorithms discussed in lectures. This assignment is\'a0going to introduce a few advanced concepts of which implementations demand a non-trivial programming expertise. As such, before reinventing the wheel, we would advise\'a0you to first explore the incredibly powerful existing Python libraries. The following two are highly recommended:\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl448\sa226\partightenfactor0
\ls1\ilvl0\cf4 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}{\field{\*\fldinst{HYPERLINK "http://scikit-learn.org/stable/"}}{\fldrslt \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 http://scikit-learn.org/stable/}}\cf6 \cb1 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec6 \
\ls1\ilvl0\cf4 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}{\field{\*\fldinst{HYPERLINK "http://pandas.pydata.org/"}}{\fldrslt \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 http://pandas.pydata.org/}}\cf6 \cb1 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec6 \
\pard\pardeftab720\sl448\sa200\partightenfactor0

\f0\b \cf6 \
\pard\pardeftab720\sl448\sa200\partightenfactor0
\cf6 \cb3 Stochastic Gradient Descent Classifier\cb1 \
\pard\pardeftab720\sl512\sa453\partightenfactor0

\f1\b0 \cf6 \cb3 In this project, we will train\'a0a Stochastic Gradient Descent Classifier. Recalled from the Machine Learning project, you were asked to implement a\'a0gradient descend update algorithm for linear regression. While gradient descend\'a0is powerful, it can\'a0be prohibitively expensive when the dataset is extremely large because\'a0every single data point \'a0needs to be processed.\cb1 \
\cb3 However, it turns out when the data is large, rather than the\'a0entire dataset, SGD algorithm performs\'a0just as good with\'a0a small random subset of the original data. This\'a0is the central\'a0idea of Stochastic SGD and\'a0particarly handy\'a0for the text data since corpus are often humongous.\cb1 \
\cb3 You should\'a0read sklearn document and\'a0learn how to use a SGD classifier. For adventurers, you are welcome to manually implement SGD yourself. Wikipedia provides a good first reference,\'a0{\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/Stochastic_gradient_descent"}}{\fldrslt \cf4 \strokec4 https://en.wikipedia.org/wiki/Stochastic_gradient_descent}}.\cb1 \
\pard\pardeftab720\sl448\sa200\partightenfactor0

\f8\i\b \cf6 \cb3 Data Preprocessing
\f0\i0 \cb1 \
\pard\pardeftab720\sl448\sa453\partightenfactor0

\f2\b0 \cf6 \cb3 The training data is provided in the directory "../resource/lib/publicdata/aclImdb/train/" of Vocareum. If you wish\'a0to download the data to your local machine for inspections, use the following link:\'a0{\field{\*\fldinst{HYPERLINK "http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"}}{\fldrslt \cf4 \strokec4 http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz}}{\field{\*\fldinst{HYPERLINK "http://ai.%20stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz."}}{\fldrslt \cf4 \strokec4 .}}\'a0\'a0
\f1 \cb1 \

\f2 \cb3 Your first task is\'a0explore this directory. There are two sub-directories\'a0
\f5\b pos/
\f2\b0 \'a0for positive texts and\'a0
\f5\b neg/\'a0
\f2\b0 for negative ones.\'a0
\f5\b You do not need to worry about unsup/, and you do not ned them.
\f2\b0 \'a0
\f1 \cb1 \

\f2 \cb3 Now\'a0
\f5\b \ul combine\'a0the raw database into a single\'a0csv\'a0files,
\f2\b0 \ulnone \'a0\'93
\f5\b imdb_\cf8 \strokec8 tr.csv
\f2\b0 \cf6 \strokec6 \'94. The\'a0
\f9 csv
\f2 \'a0file should have three\'a0columns,\'a0
\f5\b "row_number"
\f2\b0 \'a0and\'a0
\f5\b \'93text\'94 and \'93polarity\'94
\f2\b0 . The column\'a0
\f5\b \'93text\'94
\f2\b0 \'a0contains review texts from the aclImdb database and the column\'a0
\f5\b \'93polarity\'94
\f2\b0 \'a0consists of sentiment labels, 1 for positive and 0 for negative. An example of "imdb.tr.csv" is provided in the workspace.
\f1 \cb1 \
\pard\pardeftab720\sl448\sa453\partightenfactor0

\f9 \cf6 \cb3 In addition,\'a0
\f1 common English stopwords should be removed
\f10\b .\'a0
\f1\b0 An English stopwords reference are provided in your Vocareum work space for your reference. Your driver.py (as explained below)\'a0
\f0\b will have access to it during run time
\f1\b0 .\cb1 \
\pard\pardeftab720\sl448\sa200\partightenfactor0

\f8\i\b \cf6 \cb3 Unigram Data Representation
\f0\i0 \cb1 \
\pard\pardeftab720\sl448\sa453\partightenfactor0

\f2\b0 \cf6 \cb3 The very first step in solving any NLP problem is finding a way to represent the text data so that machines can understand. A common approach is using a\'a0document-term vector where each document is encoded as a discrete vector that counts occurrences of each word in the vocabulary it contains. For example, consider two one-sentence documents: \'a0
\f1 \cb1 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sl448\partightenfactor0
\ls2\ilvl1\cf6 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	
\f11 \uc0\u9702 
\f1 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec6 d1: \'93I love Columbia Artificial Intelligence course.\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl448\sa226\partightenfactor0
\ls2\ilvl0\cf6 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec6 \'a0 \'a0 d2: \'93Artificial Intelligence is awesome\'94\cb1 \
\pard\pardeftab720\sl448\sa453\partightenfactor0

\f2 \cf6 \cb3 The vocabulary V = \{artificial, awesome, Columbia, course, I, intelligence, is, love\} and two documents can be encoded as v1 and v2 as follow:
\f1 \cb1 \
\pard\pardeftab720\sl448\sa453\partightenfactor0
\cf6 \
\pard\pardeftab720\sl448\sa453\partightenfactor0

\f2 \cf6 \cb3 Hint: When building our model you should assume no access to the test data. Then what if there are words that appear only in test data but not in training data? The features will mismatch if you include those. Therefore, when extracting features in the test set, you should only\'a0
\f5\b use the vocabulary that was used in the training set.
\f1\b0 \cb1 \
\pard\pardeftab720\sl512\sa453\partightenfactor0
\cf6 \cb3 If you wish to know more, start from here\'a0{\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/Document-term_%20matrix"}}{\fldrslt \cf4 \strokec4 https://en.wikipedia.org/wiki/Document-term_ matrix}}. This data representation is also called\'a0
\f0\b \cf8 \strokec8 a unigram model
\f1\b0 \cf6 \strokec6 .\cb1 \
\pard\pardeftab720\sl448\sa453\partightenfactor0

\f2 \cf6 \cb3 Now, write a python function to transform text column in\'a0
\f5\b imdb_tr.csv
\f2\b0 \'a0into a term-document matrices using uni- gram model then\'a0train\'a0a\'a0
\f5\b Stochastic Gradient Descent (SGD) classifier
\f2\b0 \'a0whose loss=\'93hinge\'94 and penalty=\'93l1\'94 on\'a0this\'a0data.
\f1 \cb1 \

\f2 \cb3 On the other hand, in the\'a0\cf9 \strokec9 driver.py\cf6 \strokec6 , you will also find the link to\'a0
\f8\i\b "../resource/lib/publicdata/imdb_te.csv"
\f2\i0\b0 \'a0which is our\'a0benchmark file for the performance of the trained classifier. "
\f12\i\b imdb_te.csv"
\f3\b0 \'a0
\f2\i0 has two columns:\'a0
\f5\b "row_number"
\f2\b0 \'a0and\'a0
\f5\b "text".\'a0
\f2\b0 The column
\f5\b \'a0"polarity"
\f2\b0 \'a0is excluded and your job is\'a0to use the trained SGD classifier to predict this information.\'a0You should transform\'a0
\f5\b imdb_te.csv
\f2\b0 \'a0using unigram data model as well and\'a0use the trained SGD to predict the converted test set. Predictions\'a0must\'a0be formatted line by line and stored\'a0in\'a0\cf8 \strokec8 "
\f5\b unigram.output.txt"\cf6 \strokec6 \'a0
\f2\b0 in your Vocareum workspace. An example of the output file is provided for your benefits.
\f1 \cb1 \
\pard\pardeftab720\sl448\sa453\partightenfactor0

\f0\b \cf6 \cb3 If you wish to run the test in your local machine, download the following\'a0{\field{\*\fldinst{HYPERLINK "https://courses.edx.org/assets/courseware/v1/9dbe589c9a231b5174729e059a17e8eb/asset-v1:ColumbiaX+CSMM.101x+1T2020+type@asset+block/imdb_te.csv.zip"}}{\fldrslt \cf4 \strokec4 test file}}.
\f1\b0 \cb1 \
\pard\pardeftab720\sl448\sa200\partightenfactor0

\f8\i\b \cf6 \cb3 Bigram Representation
\f0\i0 \cb1 \
\pard\pardeftab720\sl448\sa453\partightenfactor0

\f9\b0 \cf6 \cb3 A more sophisticated data representation model is the bigram model where occurrences depend on a sequence of two words rather than an individual one. Taking the same example like before, v1 and v2 are now encoded as follow:
\f1 \cb1 \
\pard\pardeftab720\sl448\sa453\partightenfactor0

\f9 \cf6 \pard\pardeftab720\sl448\sa453\partightenfactor0

\f1 \cf6 \
\pard\pardeftab720\sl448\sa453\partightenfactor0

\f2 \cf6 \cb3 Instead of enumerating every individual words, bigram counts the number of instance a word following after another one. In both d1 and d2 \'93intelligence\'94 follows \'93artificial\'94 so v1(intelligence | artificial) = v2(intelligence | artificial) = 1. In contrast, \'93artificial\'94 does not follow \'93awesome\'94 so\'a0v1(artificial | awesome) = v2(artificial | awesome) = 0.\cb1 \uc0\u8232 \cb3 Repeat the same exercise from Unigram for the Bigram Model Data Representation and produce the test prediction file\'a0\cf8 \strokec8 "
\f5\b bi
\f10 gram.output.txt" .
\f1\b0 \cf6 \cb1 \strokec6 \
\pard\pardeftab720\sl448\sa200\partightenfactor0

\f12\i\b \cf6 \cb3 Tf
\f8 -
\f12 idf
\f8 :
\f0\i0 \cb1 \
\pard\pardeftab720\sl448\sa453\partightenfactor0

\f2\b0 \cf6 \cb3 Sometimes, a very high word counting may not be meaningful. For example, a common word like \'93say\'94 may appear 10 times more frequent than a less-common word such as \'93machine\'94 but it does not mean \'93say\'94 is 10 times more relevant to our\'a0sentiment classifier. To alleviate this issue, we can instead use\'a0
\f5\b \cf8 \strokec8 term
\f2\b0 \cf6 \strokec6 \'a0
\f5\b \cf8 \strokec8 frequency\cf7 \strokec7 \'a0tf[t]\'a0= 1 + log(f[t,d] )\'a0
\f2\b0 \cf6 \strokec6 where
\f5\b \cf7 \strokec7 \'a0f[t,d]
\f2\b0 \cf6 \strokec6 \'a0is the count of term t in document d. The log function dampens the unwanted influence of common English words.\cb1 \uc0\u8232 
\f1 \

\f2 \cb3 Inverse document frequency (idf) is a similar concept. To take an example, it is likely that\'a0all of our training documents belong to a same category which has specific jargons. For example, Computer Science documents often\'a0have words such as\'a0computers, CPU, programming and etc \'a0appearing over and over. While they are not common English words, because of the document domain, their occurrences are very high. To rectify, we can adjust using\'a0
\f5\b \cf8 \strokec8 inverse term frequency\'a0
\f8\i \cf7 \strokec7 idf[t] = log( N / df[t] )\'a0
\f2\i0\b0 \cf6 \strokec6 where\'a0
\f5\b df[t]
\f2\b0 \'a0is the number of documents containing the term t and N is the total number of\'a0document\'a0in the dataset.
\f1 \cb1 \

\f2 \cb3 Therefore, instead of just word frequency, tf-idf for each term t can be used,
\f5\b \'a0tf-idf[t] = tf[t] 
\f13\b0 \uc0\u8727 
\f5\b idf[t].
\f2\b0 \'a0\'a0
\f1 \cb1 \

\f2 \cb3 Repeat the same exercise as in the\'a0Unigram and Bigram data model but apply tf-idf this time to produce test prediction files,\'a0"
\f5\b \cf8 \strokec8 unigramtfidf.output.txt\cf6 \strokec6 "\'a0
\f2\b0 and\'a0"
\f5\b \cf8 \strokec8 bigramtfidf.output.txt\cf6 \strokec6 "
\f1\b0 \cb1 \
\pard\pardeftab720\sl537\sa200\partightenfactor0

\f5\b\fs38\fsmilli19200 \cf6 \cb3 II. What you need to submit:
\f0 \cb1 \
\pard\pardeftab720\sl448\sa453\partightenfactor0

\f2\b0\fs32 \cf7 \cb3 \strokec7 Your task\'a0in this assignment is to write\'a0
\f14 \cf9 driver.py
\f2 \cf7 \'a0to produce\'a0sentiment predictions over the\'a0
\f5\b \cf8 imdb_te.csv
\f2\b0 \cf7 \'a0by various\'a0text data representation (unigram, unigram with tf-idf, bigram and bigram with tf-idf). Please ensure your\'a0
\f14 \cf9 driver.py
\f2 \cf7 \'a0write the predictions to the following files\'a0
\f5\b \cf8 \ul \ulc8 during the run time
\f2\b0 \cf7 \ulnone \'a0(one-time outputs are not accepted):
\f1 \cf6 \cb1 \strokec6 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl448\sa226\partightenfactor0
\ls3\ilvl0
\f2 \cf6 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec6 unigram.output.txt
\f1 \cb1 \
\ls3\ilvl0
\f2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec6 unigramtfidf.output.txt
\f1 \cb1 \
\ls3\ilvl0
\f2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec6 bigram.output.txt
\f1 \cb1 \
\ls3\ilvl0
\f2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec6 bigramtfidf.output.txt
\f1 \cb1 \
\pard\pardeftab720\sl448\sa453\partightenfactor0

\f2 \cf6 \cb3 Be very\'a0
\f8\i\b \ul precise
\f2\i0\b0 \ulnone \'a0with these file names because the auto-grader will\'a0\cf8 \strokec8 rerun\cf6 \strokec6 \'a0your\'a0
\f14 \cf9 \strokec9 driver.py
\f2 \cf6 \strokec6 \'a0and look for them for evaluation. As usual, your program will be run as follows:
\f1 \cb1 \
\pard\pardeftab720\sl448\sa453\partightenfactor0

\f14 \cf9 \cb3 \strokec9 $python3 driver.py
\f1 \cf6 \cb1 \strokec6 \
\pard\pardeftab720\sl448\sa453\partightenfactor0

\f2 \cf6 \cb3 It is highly recommended that before submission you should perform some sanity check so you will not waste your time and opportunity to submit. Below are something you want to keep in mind:
\f1 \cb1 \
\pard\pardeftab720\sl512\sa453\partightenfactor0
\cf6 \cb3 - The name of your program file correspond with the expected, exactly\cb1 \
\cb3 - The name of the output file generated by your program\cb1 \
\cb3 - The libraries that you are using in your program be allowed (only standards lib)\cb1 \
\cb3 - The way you read the training and testing data is correct (Be aware of\'a0
\f0\b headers
\f1\b0 ! Do not get off-by-one error!)\cb1 \
\cb3 - You have performed cross validation on your model\cb1 \
\cb3 Note: \'a0Our grade will\'a0
\f0\b not\'a0
\f1\b0 call\'a0imdb_data_preprocess() ourselves. You will need to do data processing under\'a0
\f15\i if __name__ == "__main__":\'a0
\f1\i0 by yourself in the driver.\cb1 \
\pard\pardeftab720\sl537\sa200\partightenfactor0

\f5\b\fs38\fsmilli19200 \cf7 \cb3 \strokec7 III. Before you submit:\cb1 \uc0\u8232 
\f0 \cf6 \strokec6 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl448\sa226\partightenfactor0
\ls4\ilvl0
\f5\fs32 \cf8 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec8 Make sure
\f2\b0 \cf7 \strokec7 \'a0your code\'a0executes without fail on Vocareum. In particular, make sure you name your file correctly according to the instructions\'a0specified above, especially regarding different Python versions.
\f1 \cf6 \cb1 \strokec6 \
\ls4\ilvl0
\f5\b \cf8 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec7 Bonus Credits for Early Submission:\'a0
\f2\b0 \cf7 \'a0If you finish this project assignment before
\f5\b \cf9 \'a0May 3rd\'a02020 23:30 UTC,\cf7 \'a0
\f2\b0 \'a0you will get extra credits for this homework as a bonus (we count grades on your latest submission). Due to edX policy, all assignment grades are capped at 100%.
\f1 \cf6 \cb1 \strokec6 \
\ls4\ilvl0
\f5\b \cf8 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec8 You have an unlimited number of\'a0submissions
\f1\b0 \cf6 \cb1 \strokec6 \
}